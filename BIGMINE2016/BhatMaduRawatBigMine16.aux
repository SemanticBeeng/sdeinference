\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{sorensen2004parametric,iacus2009simulation,fuchs2013inference}
\citation{Wilkinson2004,Wilkinson2005}
\citation{Picchini2014}
\citation{fuchs2013inference,Elerian2001,roberts2001inference}
\citation{Archambeau2007a,Vrettas2015}
\citation{Archambeau2007,Ruttor2013}
\citation{Sarkka2015}
\citation{Donnet2008}
\citation{sorensen2004parametric,iacus2009simulation,fuchs2013inference}
\citation{BhatMadu2016}
\newlabel{jmlrstart}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.1}}
\citation{Pedersen1995}
\citation{SantaClara1997}
\citation{Sahaliaclosedform}
\citation{Picchini2014}
\citation{sorensen2004parametric,iacus2009simulation,fuchs2013inference}
\citation{Picchini2014}
\citation{Archambeau2007a,Vrettas2015}
\citation{Archambeau2007,Ruttor2013}
\citation{BhatMadu2016}
\@writefile{toc}{\contentsline {section}{\numberline {2}Statistical Method}{3}{section.0.2}}
\newlabel{sect:methods}{{2}{3}{Statistical Method}{section.0.2}{}}
\newlabel{eqn:sdefiltprob}{{1}{3}{Statistical Method}{equation.0.2.1}{}}
\newlabel{eqn:sde}{{1a}{3}{Statistical Method}{equation.0.2.1a}{}}
\newlabel{eqn:obs}{{1b}{3}{Statistical Method}{equation.0.2.1b}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Inference Problem}{3}{subsection.0.2.1}}
\newlabel{eqn:post1}{{2}{3}{Inference Problem}{equation.0.2.2}{}}
\newlabel{eqn:obsstate}{{3}{4}{Inference Problem}{equation.0.2.3}{}}
\newlabel{eqn:secondterm}{{4}{4}{Inference Problem}{equation.0.2.4}{}}
\newlabel{eqn:post2}{{5}{4}{Inference Problem}{equation.0.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Likelihood Computation via Density Tracking by Quadrature}{4}{subsection.0.2.2}}
\newlabel{sect:likelihood}{{2.2}{4}{Likelihood Computation via Density Tracking by Quadrature}{subsection.0.2.2}{}}
\newlabel{eqn:markov}{{6}{4}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.6}{}}
\newlabel{eqn:sde_em}{{7}{4}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.7}{}}
\newlabel{eqn:chapman}{{8}{5}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.8}{}}
\newlabel{eqn:kernel}{{9}{5}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.9}{}}
\newlabel{eqn:chapman2}{{10}{5}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.10}{}}
\newlabel{eqn:chapman3}{{11}{5}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.11}{}}
\newlabel{eqn:DTQfirst}{{12}{5}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.12}{}}
\newlabel{eqn:DTQlast}{{13}{5}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Metropolis Algorithm}{6}{subsection.0.2.3}}
\newlabel{sect:metropolis}{{2.3}{6}{Metropolis Algorithm}{subsection.0.2.3}{}}
\newlabel{eqn:rho}{{14}{6}{Metropolis Algorithm}{equation.0.2.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Scalable Implementation}{6}{section.0.3}}
\newlabel{sect:scalableimplementation}{{3}{6}{Scalable Implementation}{section.0.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Scala/Breeze}{7}{subsection.0.3.1}}
\newlabel{sect:scala}{{3.1}{7}{Scala/Breeze}{subsection.0.3.1}{}}
\newlabel{eqn:matrixchapman}{{15}{7}{Scala/Breeze}{equation.0.3.15}{}}
\newlabel{eqn:alldots}{{16}{8}{Scala/Breeze}{equation.0.3.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Spark}{8}{subsection.0.3.2}}
\newlabel{sect:spark}{{3.2}{8}{Spark}{subsection.0.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{8}{section.0.4}}
\newlabel{sect:results}{{4}{8}{Results}{section.0.4}{}}
\newlabel{eqn:ou}{{17}{8}{Results}{equation.0.4.17}{}}
\newlabel{eqn:ou_obs}{{18}{8}{Results}{equation.0.4.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces In order to implement the matrix-vector multiplication in (\ref  {eqn:matrixchapman}) in a scalable way, we make use of the structure of the propagator matrix $\mathcal  {G}$. Instead of computing all entries of this matrix, we compute and store only those entries that are close to the diagonal---the pink rectangles in the upper half of the diagram. The blue rectangles in the lower half of the diagram correspond to windowed versions of the pdf vector $\mathbf  {p}_i$. In both cases, there is one windowed vector per row; the row numbers go from $-M$ to $M$ as labeled. Both the pink and blue rectangles correspond to vectors of length $2\gamma +1$, with $\gamma \ll M$. The matrix-vector multiplication $\mathcal  {G} \mathbf  {p}_i$ then corresponds to a collection of $2M+1$ vector-vector dot products. This representation of (\ref  {eqn:matrixchapman}) makes efficient use of Scala, Breeze, and the Intel MKL. For more details, see the description in Section \ref  {sect:scala}.}}{9}{figure.1}}
\newlabel{fig:implementation1}{{1}{9}{In order to implement the matrix-vector multiplication in (\ref {eqn:matrixchapman}) in a scalable way, we make use of the structure of the propagator matrix $\mathcal {G}$. Instead of computing all entries of this matrix, we compute and store only those entries that are close to the diagonal---the pink rectangles in the upper half of the diagram. The blue rectangles in the lower half of the diagram correspond to windowed versions of the pdf vector $\mathbf {p}_i$. In both cases, there is one windowed vector per row; the row numbers go from $-M$ to $M$ as labeled. Both the pink and blue rectangles correspond to vectors of length $2\gamma +1$, with $\gamma \ll M$. The matrix-vector multiplication $\mathcal {G} \mathbf {p}_i$ then corresponds to a collection of $2M+1$ vector-vector dot products. This representation of (\ref {eqn:matrixchapman}) makes efficient use of Scala, Breeze, and the Intel MKL. For more details, see the description in Section \ref {sect:scala}}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces We use Spark to parallelize the computation of the likelihood (\ref  {eqn:markov}). We accomplish this by converting the original time series (for states $\mathbf  {x}$, not observations $\mathbf  {y}$) from a vector of pairs to an array where each element is a vector of consecutive pairs. The original vector of pairs is labeled as $\overrightarrow {tx}$, and the Scala \texttt  {Array} of consecutive pairs is \texttt  {tslices}. This latter object can be easily converted into a Spark RDD; subsequent \texttt  {map} operations on this RDD are executed in parallel.}}{10}{figure.2}}
\newlabel{fig:implementation2}{{2}{10}{We use Spark to parallelize the computation of the likelihood (\ref {eqn:markov}). We accomplish this by converting the original time series (for states $\mathbf {x}$, not observations $\mathbf {y}$) from a vector of pairs to an array where each element is a vector of consecutive pairs. The original vector of pairs is labeled as $\protect \overrightarrow {tx}$, and the Scala \texttt {Array} of consecutive pairs is \texttt {tslices}. This latter object can be easily converted into a Spark RDD; subsequent \texttt {map} operations on this RDD are executed in parallel}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Equispaced Time Series}{10}{subsection.0.4.1}}
\newlabel{sect:equispaced}{{4.1}{10}{Equispaced Time Series}{subsection.0.4.1}{}}
\bibdata{BhatMaduRawatBigMine16}
\bibcite{Sahaliaclosedform}{{1}{2002}{{A\"it-Sahalia}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Posterior densities for equispaced time series}}{11}{figure.3}}
\newlabel{fig:post_equi}{{3}{11}{Posterior densities for equispaced time series}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Non-equispaced Time Series}{11}{subsection.0.4.2}}
\newlabel{sect:nonequispaced}{{4.2}{11}{Non-equispaced Time Series}{subsection.0.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{11}{section.0.5}}
\newlabel{sect:conclusion}{{5}{11}{Conclusion}{section.0.5}{}}
\bibcite{Archambeau2007}{{2}{2007{a}}{{Archambeau et~al.}}{{Archambeau, Cornford, Opper, and Shawe-Taylor}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Posterior densities for non-equispaced time series}}{12}{figure.4}}
\newlabel{fig:post_nonequi}{{4}{12}{Posterior densities for non-equispaced time series}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Observations and inferred states}}{12}{figure.5}}
\newlabel{fig:timeseries1}{{5}{12}{Observations and inferred states}{figure.5}{}}
\bibcite{Archambeau2007a}{{3}{2007{b}}{{Archambeau et~al.}}{{Archambeau, Opper, Shen, Cornford, and Shawe-Taylor}}}
\bibcite{BhatMadu2016}{{4}{2016}{{Bhat and Madushani}}{{}}}
\bibcite{Donnet2008}{{5}{2008}{{Donnet and Samson}}{{}}}
\bibcite{Elerian2001}{{6}{2001}{{Elerian et~al.}}{{Elerian, Chib, and Shephard}}}
\bibcite{fuchs2013inference}{{7}{2013}{{Fuchs}}{{}}}
\bibcite{Wilkinson2004}{{8}{2004}{{Golightly and Wilkinson}}{{}}}
\bibcite{Wilkinson2005}{{9}{2005}{{Golightly and Wilkinson}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Observations and mean inferred state with error bars. The error bars are computed by adding/subtracting the mean inferred value of $\sigma _\epsilon $ to the observation series $\mathbf  {y}$.}}{13}{figure.6}}
\newlabel{fig:timeseries2}{{6}{13}{Observations and mean inferred state with error bars. The error bars are computed by adding/subtracting the mean inferred value of $\sigma _\epsilon $ to the observation series $\mathbf {y}$}{figure.6}{}}
\bibcite{iacus2009simulation}{{10}{2009}{{Iacus}}{{}}}
\bibcite{Pedersen1995}{{11}{1995}{{Pedersen}}{{}}}
\bibcite{Picchini2014}{{12}{2014}{{Picchini}}{{}}}
\bibcite{roberts2001inference}{{13}{2001}{{Roberts and Stramer}}{{}}}
\bibcite{Ruttor2013}{{14}{2013}{{Ruttor et~al.}}{{Ruttor, Batz, and Opper}}}
\bibcite{SantaClara1997}{{15}{1997}{{Santa-Clara}}{{}}}
\bibcite{Sarkka2015}{{16}{2015}{{S\"{a}rkk\"{a} et~al.}}{{S\"{a}rkk\"{a}, Hartikainen, Mbalawata, and Haario}}}
\bibcite{sorensen2004parametric}{{17}{2004}{{S{\o }rensen}}{{}}}
\bibcite{Vrettas2015}{{18}{2015}{{Vrettas et~al.}}{{Vrettas, Opper, and Cornford}}}
\newlabel{jmlrend}{{5}{14}{end of Scalable SDE Filtering and Inference}{section*.3}{}}

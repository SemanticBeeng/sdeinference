\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{Kneissler2015}
\citation{Sun2008}
\citation{chen2003,sorensen2004parametric,iacus2009simulation,barber2011bayesian,fuchs2013inference}
\newlabel{jmlrstart}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.1}}
\newlabel{sect:intro}{{1}{1}{Introduction}{section.0.1}{}}
\citation{pomp}
\citation{Pedersen1995}
\citation{SantaClara1997}
\citation{Sahaliaclosedform}
\citation{Picchini2014}
\citation{Archambeau2007a,Vrettas2015}
\citation{Archambeau2007,Ruttor2013}
\@writefile{toc}{\contentsline {section}{\numberline {2}Statistical Method}{3}{section.0.2}}
\newlabel{sect:methods}{{2}{3}{Statistical Method}{section.0.2}{}}
\newlabel{eqn:sdefiltprob}{{1}{3}{Statistical Method}{equation.0.2.1}{}}
\newlabel{eqn:sde}{{1a}{3}{Statistical Method}{equation.0.2.1a}{}}
\newlabel{eqn:obs}{{1b}{3}{Statistical Method}{equation.0.2.1b}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Inference Problem}{3}{subsection.0.2.1}}
\newlabel{eqn:post1}{{2}{3}{Inference Problem}{equation.0.2.2}{}}
\citation{BhatMadu2016}
\newlabel{eqn:obsstate}{{3}{4}{Inference Problem}{equation.0.2.3}{}}
\newlabel{eqn:secondterm}{{4}{4}{Inference Problem}{equation.0.2.4}{}}
\newlabel{eqn:post2}{{5}{4}{Inference Problem}{equation.0.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Likelihood Computation via Density Tracking by Quadrature}{4}{subsection.0.2.2}}
\newlabel{sect:likelihood}{{2.2}{4}{Likelihood Computation via Density Tracking by Quadrature}{subsection.0.2.2}{}}
\newlabel{eqn:markov}{{6}{4}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.6}{}}
\newlabel{eqn:sde_em}{{7}{4}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.7}{}}
\newlabel{eqn:chapman}{{8}{5}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.8}{}}
\newlabel{eqn:kernel}{{9}{5}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.9}{}}
\newlabel{eqn:chapman2}{{10}{5}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.10}{}}
\newlabel{eqn:chapman3}{{11}{5}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.11}{}}
\newlabel{eqn:DTQfirst}{{12}{5}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.12}{}}
\newlabel{eqn:DTQlast}{{13}{5}{Likelihood Computation via Density Tracking by Quadrature}{equation.0.2.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Metropolis Algorithm}{6}{subsection.0.2.3}}
\newlabel{sect:metropolis}{{2.3}{6}{Metropolis Algorithm}{subsection.0.2.3}{}}
\newlabel{eqn:rho}{{14}{6}{Metropolis Algorithm}{equation.0.2.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Scalable Implementation}{6}{section.0.3}}
\newlabel{sect:scalableimplementation}{{3}{6}{Scalable Implementation}{section.0.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Scala/Breeze}{6}{subsection.0.3.1}}
\newlabel{sect:scala}{{3.1}{6}{Scala/Breeze}{subsection.0.3.1}{}}
\newlabel{eqn:matrixchapman}{{15}{6}{Scala/Breeze}{equation.0.3.15}{}}
\newlabel{eqn:alldots}{{16}{7}{Scala/Breeze}{equation.0.3.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Spark}{8}{subsection.0.3.2}}
\newlabel{sect:spark}{{3.2}{8}{Spark}{subsection.0.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{8}{section.0.4}}
\newlabel{sect:results}{{4}{8}{Results}{section.0.4}{}}
\newlabel{eqn:ou}{{17}{8}{Results}{equation.0.4.17}{}}
\newlabel{eqn:ou_obs}{{18}{8}{Results}{equation.0.4.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces In order to implement the matrix-vector multiplication in (\ref  {eqn:matrixchapman}) in a scalable way, we make use of the structure of the propagator matrix $\mathcal  {G}$. Instead of computing all entries of this matrix, we compute and store only those entries that are close to the diagonal---the pink rectangles in the upper half of the diagram. The blue rectangles in the lower half of the diagram correspond to windowed versions of the pdf vector $\mathbf  {p}_i$. In both cases, there is one windowed vector per row; the row numbers go from $-M$ to $M$ as labeled. Both the pink and blue rectangles correspond to vectors of length $2\gamma +1$, with $\gamma \ll M$. The matrix-vector multiplication $\mathcal  {G} \mathbf  {p}_i$ then corresponds to a collection of $2M+1$ vector-vector dot products. This representation of (\ref  {eqn:matrixchapman}) makes efficient use of Scala, Breeze, and the Intel MKL. For more details, see the description in Section \ref  {sect:scala}.}}{9}{figure.1}}
\newlabel{fig:implementation1}{{1}{9}{In order to implement the matrix-vector multiplication in (\ref {eqn:matrixchapman}) in a scalable way, we make use of the structure of the propagator matrix $\mathcal {G}$. Instead of computing all entries of this matrix, we compute and store only those entries that are close to the diagonal---the pink rectangles in the upper half of the diagram. The blue rectangles in the lower half of the diagram correspond to windowed versions of the pdf vector $\mathbf {p}_i$. In both cases, there is one windowed vector per row; the row numbers go from $-M$ to $M$ as labeled. Both the pink and blue rectangles correspond to vectors of length $2\gamma +1$, with $\gamma \ll M$. The matrix-vector multiplication $\mathcal {G} \mathbf {p}_i$ then corresponds to a collection of $2M+1$ vector-vector dot products. This representation of (\ref {eqn:matrixchapman}) makes efficient use of Scala, Breeze, and the Intel MKL. For more details, see the description in Section \ref {sect:scala}}{figure.1}{}}
\citation{Scott2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces We use Spark to parallelize the computation of the likelihood (\ref  {eqn:markov}). We accomplish this by converting the original time series (for states $\mathbf  {x}$, not observations $\mathbf  {y}$) from a vector of pairs to an array where each element is a vector of consecutive pairs. The original vector of pairs is labeled as $\overrightarrow {tx}$, and the Scala \texttt  {Array} of consecutive pairs is \texttt  {tslices}. This latter object can be easily converted into a Spark RDD; subsequent \texttt  {map} operations on this RDD are executed in parallel.}}{10}{figure.2}}
\newlabel{fig:implementation2}{{2}{10}{We use Spark to parallelize the computation of the likelihood (\ref {eqn:markov}). We accomplish this by converting the original time series (for states $\mathbf {x}$, not observations $\mathbf {y}$) from a vector of pairs to an array where each element is a vector of consecutive pairs. The original vector of pairs is labeled as $\protect \overrightarrow {tx}$, and the Scala \texttt {Array} of consecutive pairs is \texttt {tslices}. This latter object can be easily converted into a Spark RDD; subsequent \texttt {map} operations on this RDD are executed in parallel}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Equispaced Time Series}{10}{subsection.0.4.1}}
\newlabel{sect:equispaced}{{4.1}{10}{Equispaced Time Series}{subsection.0.4.1}{}}
\citation{pomp}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Posterior densities for the inference/filtering problem with equispaced time series $(\mathbf  {t},\mathbf  {y})$. Each density is calculated on the basis of $9900$ post-burn-in Metropolis samples computed using the indicated value of the internal DTQ time step parameter $h$. Overall, we see reasonable agreement between the ground truth values (indicated by red vertical lines) and the posterior densities.}}{11}{figure.3}}
\newlabel{fig:post_equi}{{3}{11}{Posterior densities for the inference/filtering problem with equispaced time series $(\mathbf {t},\mathbf {y})$. Each density is calculated on the basis of $9900$ post-burn-in Metropolis samples computed using the indicated value of the internal DTQ time step parameter $h$. Overall, we see reasonable agreement between the ground truth values (indicated by red vertical lines) and the posterior densities}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Non-equispaced Time Series}{11}{subsection.0.4.2}}
\newlabel{sect:nonequispaced}{{4.2}{11}{Non-equispaced Time Series}{subsection.0.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Posterior densities for the inference/filtering problem with non-equispaced time series $(\mathbf  {t},\mathbf  {y})$. Each density is calculated on the basis of $9900$ post-burn-in Metropolis samples computed using the indicated value of the internal DTQ time step parameter $h$. Overall, we see reasonable agreement between the ground truth values (indicated by red vertical lines) and the posterior densities.}}{12}{figure.4}}
\newlabel{fig:post_nonequi}{{4}{12}{Posterior densities for the inference/filtering problem with non-equispaced time series $(\mathbf {t},\mathbf {y})$. Each density is calculated on the basis of $9900$ post-burn-in Metropolis samples computed using the indicated value of the internal DTQ time step parameter $h$. Overall, we see reasonable agreement between the ground truth values (indicated by red vertical lines) and the posterior densities}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces We plot the observations (in red) together with each of the samples of the state series $\mathbf  {x}$. Each such sample is a grey curve, and the mean of all such grey curves is plotted in black. We refer to the black curve as the mean inferred state series.}}{13}{figure.5}}
\newlabel{fig:timeseries1}{{5}{13}{We plot the observations (in red) together with each of the samples of the state series $\mathbf {x}$. Each such sample is a grey curve, and the mean of all such grey curves is plotted in black. We refer to the black curve as the mean inferred state series}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces We plot the observations (in red) together with the mean inferred state series (in black). The error bars (grey) are computed by adding/subtracting the mean inferred value of $\sigma _\epsilon $ to/from the observation series $\mathbf  {y}$. Note that the mean inferred state is typically within one $\sigma _\epsilon $ of the corresponding observation.}}{13}{figure.6}}
\newlabel{fig:timeseries2}{{6}{13}{We plot the observations (in red) together with the mean inferred state series (in black). The error bars (grey) are computed by adding/subtracting the mean inferred value of $\sigma _\epsilon $ to/from the observation series $\mathbf {y}$. Note that the mean inferred state is typically within one $\sigma _\epsilon $ of the corresponding observation}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Scaling}{14}{subsection.0.4.3}}
\newlabel{sect:scaling}{{4.3}{14}{Scaling}{subsection.0.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Left panel}: For each indicated value of $L$, we have generated a time series of length $L$, run our inference/filtering code, and recorded the amount of time $T$ required to generate $1000$ Metropolis samples of the posterior. We fit lines to $\qopname  \relax o{log}T$ as a function of $\qopname  \relax o{log}L$---both the lines and the original data are plotted on log-transformed axes. The slopes of the lines are less than $1$, consistent with $O(L)$ temporal scaling. \textbf  {Right panel}: For a non-equispaced time series of length $2501$, we ran our code with $\nu $ Spark processors where $\nu \in \{3,6,12,24\}$. We recorded $T$, the time required to generate $10$ Metropolis samples of the posterior. We fit lines to $\qopname  \relax o{log}T$ as a function of $\qopname  \relax o{log}\nu $---both the lines and the original data are plotted on log-transformed axes. The slopes of the lines are close to $-0.5$, suggesting $O(\nu ^{-1/2})$ scaling.}}{14}{figure.7}}
\newlabel{fig:scaling}{{7}{14}{\textbf {Left panel}: For each indicated value of $L$, we have generated a time series of length $L$, run our inference/filtering code, and recorded the amount of time $T$ required to generate $1000$ Metropolis samples of the posterior. We fit lines to $\log T$ as a function of $\log L$---both the lines and the original data are plotted on log-transformed axes. The slopes of the lines are less than $1$, consistent with $O(L)$ temporal scaling. \textbf {Right panel}: For a non-equispaced time series of length $2501$, we ran our code with $\nu $ Spark processors where $\nu \in \{3,6,12,24\}$. We recorded $T$, the time required to generate $10$ Metropolis samples of the posterior. We fit lines to $\log T$ as a function of $\log \nu $---both the lines and the original data are plotted on log-transformed axes. The slopes of the lines are close to $-0.5$, suggesting $O(\nu ^{-1/2})$ scaling}{figure.7}{}}
\citation{fuchs2013inference}
\bibdata{BhatMaduRawatBigMine16}
\bibcite{Sahaliaclosedform}{{1}{2002}{{A\"it-Sahalia}}{{}}}
\bibcite{Archambeau2007}{{2}{2007{a}}{{Archambeau et~al.}}{{Archambeau, Cornford, Opper, and Shawe-Taylor}}}
\bibcite{Archambeau2007a}{{3}{2007{b}}{{Archambeau et~al.}}{{Archambeau, Opper, Shen, Cornford, and Shawe-Taylor}}}
\bibcite{barber2011bayesian}{{4}{2011}{{Barber et~al.}}{{Barber, Cemgil, and Chiappa}}}
\bibcite{BhatMadu2016}{{5}{2016}{{Bhat and Madushani}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion and Conclusion}{15}{section.0.5}}
\newlabel{sect:conclusion}{{5}{15}{Discussion and Conclusion}{section.0.5}{}}
\bibcite{chen2003}{{6}{2003}{{Chen}}{{}}}
\bibcite{fuchs2013inference}{{7}{2013}{{Fuchs}}{{}}}
\bibcite{iacus2009simulation}{{8}{2009}{{Iacus}}{{}}}
\bibcite{pomp}{{9}{}{{King et~al.}}{{King, Nguyen, and Ionides}}}
\bibcite{Kneissler2015}{{10}{2015}{{Kneissler et~al.}}{{Kneissler, Drugowitsch, Friston, and Butz}}}
\bibcite{Pedersen1995}{{11}{1995}{{Pedersen}}{{}}}
\bibcite{Picchini2014}{{12}{2014}{{Picchini}}{{}}}
\bibcite{Ruttor2013}{{13}{2013}{{Ruttor et~al.}}{{Ruttor, Batz, and Opper}}}
\bibcite{SantaClara1997}{{14}{1997}{{Santa-Clara}}{{}}}
\bibcite{Scott2015}{{15}{2015}{{Scott}}{{}}}
\bibcite{sorensen2004parametric}{{16}{2004}{{S{\o }rensen}}{{}}}
\bibcite{Sun2008}{{17}{2008}{{Sun et~al.}}{{Sun, Jin, and Xiong}}}
\bibcite{Vrettas2015}{{18}{2015}{{Vrettas et~al.}}{{Vrettas, Opper, and Cornford}}}
\newlabel{jmlrend}{{5}{16}{end of Scalable SDE Filtering and Inference}{section*.3}{}}
